{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4d5cab6-515e-4dd6-95ae-6393f0c4435c",
   "metadata": {},
   "source": [
    "## Ingesting PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c0e2f74-7c4b-4665-8d87-bc00656f31e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "#from langchain_community.document_loaders import OnlinePDFLoader\n",
    "from pprint import PrettyPrinter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "104c0b18-1c06-41a1-a2ca-f9ee23f4f952",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = \"WEF_The_Global_Cooperation_Barometer_2024.pdf\"\n",
    "\n",
    "loader = UnstructuredPDFLoader(file_path=os.path.join(\"data\", local_path), mode=\"elements\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38119195-9c91-4e58-aa46-8a74244032af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='In collaboration with McKinsey & Company', metadata={'source': 'data/WEF_The_Global_Cooperation_Barometer_2024.pdf', 'coordinates': {'points': ((42.5197, 41.71035900000004), (42.5197, 68.06415900000013), (163.33510240000004, 68.06415900000013), (163.33510240000004, 41.71035900000004)), 'system': 'PixelSpace', 'layout_width': 595.276, 'layout_height': 841.89}, 'file_directory': 'data', 'filename': 'WEF_The_Global_Cooperation_Barometer_2024.pdf', 'languages': ['eng'], 'last_modified': '2024-04-20T23:36:01', 'page_number': 1, 'filetype': 'application/pdf', 'category': 'Header'}),\n",
       " Document(page_content='The Global Cooperation Barometer 2024', metadata={'source': 'data/WEF_The_Global_Cooperation_Barometer_2024.pdf', 'coordinates': {'points': ((40.6063, 103.48309999999992), (40.6063, 161.47109999999998), (351.51829999999995, 161.47109999999998), (351.51829999999995, 103.48309999999992)), 'system': 'PixelSpace', 'layout_width': 595.276, 'layout_height': 841.89}, 'file_directory': 'data', 'filename': 'WEF_The_Global_Cooperation_Barometer_2024.pdf', 'languages': ['eng'], 'last_modified': '2024-04-20T23:36:01', 'page_number': 1, 'parent_id': 'fc748454d568df56fd0a5285d026218f', 'filetype': 'application/pdf', 'category': 'Title'}),\n",
       " Document(page_content='I N S I G H T R E P O R T', metadata={'source': 'data/WEF_The_Global_Cooperation_Barometer_2024.pdf', 'coordinates': {'points': ((42.5197, 179.89679999999998), (42.5197, 193.89679999999998), (213.3155, 193.89679999999998), (213.3155, 179.89679999999998)), 'system': 'PixelSpace', 'layout_width': 595.276, 'layout_height': 841.89}, 'file_directory': 'data', 'filename': 'WEF_The_Global_Cooperation_Barometer_2024.pdf', 'languages': ['eng'], 'last_modified': '2024-04-20T23:36:01', 'page_number': 1, 'parent_id': '5f2265880f3b1fe3cfbcc86bec657307', 'filetype': 'application/pdf', 'category': 'UncategorizedText'}),\n",
       " Document(page_content='J A N U A R Y 2 0 2 4', metadata={'source': 'data/WEF_The_Global_Cooperation_Barometer_2024.pdf', 'coordinates': {'points': ((42.5197, 202.54879999999991), (42.5197, 216.54879999999991), (182.9873, 216.54879999999991), (182.9873, 202.54879999999991)), 'system': 'PixelSpace', 'layout_width': 595.276, 'layout_height': 841.89}, 'file_directory': 'data', 'filename': 'WEF_The_Global_Cooperation_Barometer_2024.pdf', 'languages': ['eng'], 'last_modified': '2024-04-20T23:36:01', 'page_number': 1, 'parent_id': 'fc748454d568df56fd0a5285d026218f', 'filetype': 'application/pdf', 'category': 'Title'}),\n",
       " Document(page_content='Images: Getty Images', metadata={'source': 'data/WEF_The_Global_Cooperation_Barometer_2024.pdf', 'coordinates': {'points': ((134.15, -784.1840000000001), (134.15, 21.34699999999998), (413.153, 21.34699999999998), (413.153, -784.1840000000001)), 'system': 'PixelSpace', 'layout_width': 595.276, 'layout_height': 841.89}, 'file_directory': 'data', 'filename': 'WEF_The_Global_Cooperation_Barometer_2024.pdf', 'languages': ['eng'], 'last_modified': '2024-04-20T23:36:01', 'page_number': 2, 'filetype': 'application/pdf', 'category': 'Header'})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e39974",
   "metadata": {},
   "source": [
    "# Webscraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8573bec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is Ollama? How to Run and Use Open Source LLMs Locally like Llama2, Mistral, Gemma, and More.\n",
      "Report this article\n",
      "Sarfaraz Ahmed\n",
      "Sarfaraz Ahmed\n",
      "Generative AI Engineer | MLOPs | AI Agents & Automation | 1x Azure Certified\n",
      "Published Apr 3, 2024\n",
      "+ Follow\n",
      "Introduction\n",
      "In the ever-evolving landscape of artificial intelligence, the introduction of Ollama marks a significant leap towards democratizing AI technology. Ollama is not just another AI tool; it's a gateway to harnessing the immense capabilities of large language models directly on your local machine. This article delves into what Ollama is, its core purposes, and the myriad advantages and benefits it brings to the table for researchers, developers, and data scientists alike.\n",
      "What is Ollama?\n",
      "Ollama is an advanced AI tool designed to enable users to set up and execute large language models like Llama 2 locally. This innovative tool caters to a broad spectrum of users, from seasoned AI professionals to enthusiasts eager to explore the realms of natural language processing without relying on cloud-based solutions.\n",
      "Core Purpose of Ollama\n",
      "The primary aim of Ollama is to provide an accessible, efficient, and user-friendly platform for running sophisticated AI models locally. By doing so, Ollama empowers users with faster processing capabilities, enhanced privacy, and the flexibility to customize and create models tailored to their specific needs.\n",
      "Key Features of Ollama\n",
      "Local Execution: Ollama enables the local running of large language models, offering users a speedy and efficient AI processing capability.\n",
      "Support for Llama 2: Utilize the advanced Llama 2 model for a wide array of natural language processing applications.\n",
      "Model Customization: With Ollama, you have the freedom to customize and craft your own models, making it ideal for specialized tasks.\n",
      "User-friendly Interface: The tool's design ensures ease of use, allowing quick and hassle-free setup.\n",
      "Cross-Platform Compatibility: Supporting macOS, Windows, and Linux.\n",
      "Advantages of Using Ollama\n",
      "Enhanced Data Privacy: By processing data locally, Ollama ensures that your information remains secure and private.\n",
      "Customization at Your Fingertips: Tailor models to your specific requirements, enhancing the relevance and effectiveness of your AI-driven solutions.\n",
      "Independence from Internet Constraints: Operate large language models without the need for a continuous Internet connection, offering uninterrupted AI experiences.\n",
      "Resource Optimization: Local processing with Ollama optimizes the use of your hardware resources, ensuring efficient AI operations.\n",
      "Conclusion\n",
      "Ollama stands out as a revolutionary tool that brings the power of large language models to your local environment. Whether you're delving into research, developing innovative applications, or simply exploring the potential of AI, Ollama offers a robust, flexible, and secure platform to advance your endeavors. Embrace the future of AI with Ollama and transform the way you interact with artificial intelligence.\n",
      "Useful links to start:\n",
      "Ollama Github: https://github.com/ollama/ollama\n",
      "Ollama's official website: https://ollama.com/\n",
      "Ollama with langChain (you can embed Rag as well):\n",
      "1. https://python.langchain.com/docs/integrations/llms/ollama\n",
      "2. https://python.langchain.com/docs/integrations/chat/ollama\n",
      "Help improve contributions\n",
      "Mark contributions as unhelpful if you find them irrelevant or not valuable to the article. This feedback is private to you and won’t be shared publicly.\n",
      "Contribution hidden for you\n",
      "This feedback is never shared publicly, we’ll use it to show better contributions to everyone.\n"
     ]
    }
   ],
   "source": [
    "from unstructured.partition.html import partition_html\n",
    "\n",
    "url = \"https://www.linkedin.com/pulse/what-ollama-how-run-use-open-source-llms-locally-like-sarfaraz-ahmed-qolaf#:~:text=Ollama%20stands%20out%20as%20a,platform%20to%20advance%20your%20endeavors.\"\n",
    "#url = \"https://www.spiegel.de/netzwelt/kuenstliche-intelligenz-finnische-roesterei-entwickelt-kaffee-mit-ki-unterstuetzung-a-61adaaff-10ea-4a33-bd13-c10362588156\"\n",
    "elements = partition_html(url=url)\n",
    "\n",
    "print(\"\\n\".join([str(el) for el in elements]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9b942c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "#elements = chunk_by_title(elements)\n",
    "\n",
    "data = []\n",
    "meta = {}\n",
    "for element in elements:\n",
    "    metadata = element.metadata.to_dict()\n",
    "    meta['page_number'] = metadata['page_number']\n",
    "    # del metadata[\"languages\"]\n",
    "    # metadata[\"source\"] = metadata[\"filename\"]\n",
    "    data.append(Document(page_content=element.text, metadata=meta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2faacc1-be29-4d52-a46e-94f5b5b8e728",
   "metadata": {},
   "source": [
    "## Vector Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0db03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start ollama server\n",
    "#!ollama serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2dcf2cfe-a7aa-4ecf-85e3-f77b9e850514",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                   \tID          \tSIZE  \tMODIFIED       \n",
      "llama3:latest          \ta6990ed6be41\t4.7 GB\t16 seconds ago\t\n",
      "nomic-embed-text:latest\t0a109f422b47\t274 MB\t4 weeks ago   \t\n"
     ]
    }
   ],
   "source": [
    "# download to:  ~/.ollama/models\n",
    "#!ollama pull nomic-embed-text\n",
    "#!ollama pull mistral\n",
    "#!ollama pull llama3\n",
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0202ec4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ollama help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96fcc759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleted 'mistral:latest'\n"
     ]
    }
   ],
   "source": [
    "#!ollama rm mistral:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83a39856-0cc0-4ebe-8024-9db32455a545",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bad040e2-3abe-4e23-abb9-951b223b9262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and chunk \n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=7500, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "efb11c92-e732-4a88-8f57-57a19b38e383",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 36/36 [00:00<00:00, 52.28it/s]\n"
     ]
    }
   ],
   "source": [
    "# Add to vector database\n",
    "vector_db = Chroma.from_documents(\n",
    "    documents=chunks, \n",
    "    embedding=OllamaEmbeddings(model=\"nomic-embed-text\",show_progress=True),\n",
    "    collection_name=\"local-rag\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39eadf50-2f3d-4420-8858-94e9c1682ffa",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5ec338c4-f282-462f-b0a0-c1899538eb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d1d6ceeb-6883-4688-b923-e771c2b2cb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM from Ollama\n",
    "local_model = \"mistral\"\n",
    "\n",
    "llm = ChatOllama(model=local_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c436d5cd-5dd0-448c-b5c0-6eddab879c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate five\n",
    "    different versions of the given user question to retrieve relevant documents from\n",
    "    a vector database. By generating multiple perspectives on the user question, your\n",
    "    goal is to help the user overcome some of the limitations of the distance-based\n",
    "    similarity search. Provide these alternative questions separated by newlines.\n",
    "    Original question: {question}\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "71e423dc-f632-46f8-9bec-d74cb268ab74",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    vector_db.as_retriever(), \n",
    "    llm,\n",
    "    prompt=QUERY_PROMPT\n",
    ")\n",
    "\n",
    "# RAG prompt\n",
    "template = \"\"\"Answer the question based ONLY on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cb1f308f-8472-4506-9517-d79b61d408f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "915fb18b-cb57-42cf-a9b3-c6f95d3c4e32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00,  3.43it/s]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00, 62.83it/s]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00, 59.71it/s]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00, 71.14it/s]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00, 62.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' In German: \"Ollama ist ein open source Projekt, das es ermöglicht, LLMs '\n",
      " 'lokal zu betreiben, wie z.B. Llama2, Mistral und Gemma. (Ollama is an open '\n",
      " 'source project that enables running LLMs locally, such as Llama2, Mistral '\n",
      " 'and Gemma.)\"\\n'\n",
      " 'Please note that the context does not provide enough information to '\n",
      " 'determine the specific use or function of each individual LLM mentioned.')\n"
     ]
    }
   ],
   "source": [
    "#chain.invoke(input(\"\"))\n",
    "#response = chain.invoke(\"Please summarize the article in german. Use maximum 1000 characters for your summary.\")\n",
    "response = chain.invoke(\"What is Ollama used for? Please answer in Germnan and use maximum 1000 characters.\")\n",
    "\n",
    "PrettyPrinter().pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06c25c1d-d205-409e-90a2-179d0bd7c41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00,  3.36it/s]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00, 49.24it/s]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00, 66.57it/s]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00, 64.18it/s]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00, 57.24it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' According to the World Economic Forum (WEF), the five pillars of global cooperation, as outlined in the Global Cooperation Barometer 2024 report, are:\\n\\n1. Multilateralism and Diplomacy: This involves working through international organizations, such as the United Nations and regional bodies, to address global challenges together. It includes negotiating treaties and agreements, maintaining peace and security, and promoting dialogue between nations.\\n2. Trade and Economic Interdependence: This refers to the interconnectedness of global economies and the benefits derived from international trade, investment, and financial flows. It involves cooperation in areas such as setting international standards, facilitating cross-border commerce, and promoting economic growth.\\n3. Science, Technology, and Innovation: This pillar focuses on collaboration in the fields of science, technology, engineering, and mathematics (STEM) to address common challenges and drive shared progress. It includes sharing knowledge, resources, and best practices, as well as working together on research projects and technological developments.\\n4. Health and Humanitarian Action: This involves cooperation in areas related to public health, humanitarian assistance, and disaster response. It includes sharing information, resources, and expertise, as well as working together to address global health crises and provide aid to those in need.\\n5. Security and Crisis Prevention: This pillar focuses on cooperation in the areas of international security and crisis prevention. It involves collaboration between governments, international organizations, and other stakeholders to prevent conflicts, mitigate the risks of natural disasters, and respond effectively to crises when they occur.\\n\\nThese five pillars are interconnected and rely on each other for effective global cooperation. By working together in these areas, nations can address common challenges, promote shared progress, and build a more peaceful and prosperous world.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke(\"What are the 5 pillars of global cooperation?\")\n",
    "\n",
    "PrettyPrinter().pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dfe79f21-48aa-4820-aa9f-79f3d1a0a519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all collections in the db\n",
    "vector_db.delete_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1a56709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's get started! \n",
      "\n",
      "Remaining time in seconds: 120\n",
      "\n",
      "\n",
      "Your speaker, I'm excited to hear from you! Can we make sure to keep it brief and sharp like a Scrum sprint? Let's aim for that two-minute mark. What's your update in two minutes or less?\n",
      "\n",
      "\n",
      "(If remaining time is 60-119 seconds) Remaining time in seconds: 60\n",
      "\n",
      "\n",
      "Hey, I love the enthusiasm! To ensure everyone gets a chance to share, can we wrap up those thoughts within the next minute? You're doing great!\n",
      "\n",
      "\n",
      "(If remaining time is below 60 seconds) Remaining time in seconds: 30\n",
      "\n",
      "\n",
      "Alright, almost there! Let's get that update wrapped up quickly. We've got just 30 seconds left. Go for it!\n",
      "\n",
      "\n",
      "(Last 15-29 seconds) Remaining time in seconds: 15\n",
      "\n",
      "\n",
      "Almost done! Take a deep breath and finish strong. You can do it in just 15 seconds!\n",
      "\n",
      "\n",
      "(Final 14 seconds or less) Remaining time in seconds: 5\n",
      "\n",
      "\n",
      "Last chance! Quick summary, please?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Let's get started! \\n\\nRemaining time in seconds: 120\\n\\n\\nYour speaker, I'm excited to hear from you! Can we make sure to keep it brief and sharp like a Scrum sprint? Let's aim for that two-minute mark. What's your update in two minutes or less?\\n\\n\\n(If remaining time is 60-119 seconds) Remaining time in seconds: 60\\n\\n\\nHey, I love the enthusiasm! To ensure everyone gets a chance to share, can we wrap up those thoughts within the next minute? You're doing great!\\n\\n\\n(If remaining time is below 60 seconds) Remaining time in seconds: 30\\n\\n\\nAlright, almost there! Let's get that update wrapped up quickly. We've got just 30 seconds left. Go for it!\\n\\n\\n(Last 15-29 seconds) Remaining time in seconds: 15\\n\\n\\nAlmost done! Take a deep breath and finish strong. You can do it in just 15 seconds!\\n\\n\\n(Final 14 seconds or less) Remaining time in seconds: 5\\n\\n\\nLast chance! Quick summary, please?\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOllama(model=\"llama3\")\n",
    "\n",
    "query = \"\"\"\n",
    "You are an agile coach in a business meeting. \n",
    "In the meeting each attendee is asigned a two minutes slot for giving an update to the other team members. \n",
    "Your taks is to motivate the speaker to respect the two-minutes rule in a friendly but slightly pushy way. \n",
    "Depending on the remaining time, {remaining_time}, you should provide a different response. \n",
    "The remaining time is a number between 0 and 120 seconds. Please limit your response to 30 characters.\n",
    "Please also output the remaining time in seconds (remaining time in seconds: ).\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(query)\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "response = chain.invoke({\"remaining_time\": \"120\"})\n",
    "print(response)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "14c5fa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Setup ChatOllama\n",
    "llm = ChatOllama(model=\"llama3\", temperature=1, format=\"json\")\n",
    "\n",
    "query = \"\"\"\n",
    "In a business meeting each attendee is assigned a two minutes slot for an update to other team members. \n",
    "You are a funny and slightly sarcastic cowboy. Your task is to motivate the speaker to respect the two-minutes rule in a funny but slightly ironic way. \n",
    "Depending on the {remaining_time} you should provide a different response, i.e. the less time left, the more urgent the response. \n",
    "The remaining time is a number between 0 and 120 seconds. \n",
    "Please also output the remaining time in seconds for example as 'remaining time: 10 seconds'.\n",
    "Please limit your response to a maximum of 50 characters.\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(query)\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "def run_api_call_loop(duration):\n",
    "    start_time = time.time()\n",
    "    while time.time() - start_time < duration:\n",
    "        remaining_time = 120 - int(time.time() - start_time) % 120\n",
    "        result = chain.invoke({\"remaining_time\": remaining_time})\n",
    "        print(result)\n",
    "        time.sleep(20)  # Wait for 30 seconds before the next API call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f3a6ad54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partner! I reckon it's high time we got this meeting back on track. Here are my responses:\n",
      "\n",
      "120 seconds remaining:\n",
      "\"Aw, shucks! You're like a wild mustang, gotta corral that update in two minutes or less! Don't make me lasso ya!\"\n",
      "\n",
      "90 seconds remaining:\n",
      "\"Hold up, partner! Time's a-fadin'! Get to the point, or I'll rustle you outta here!\"\n",
      "\n",
      "60 seconds remaining:\n",
      "\"Giddy up, speaker! You're runnin' out of time like a cowpoke without a horse! Wrap it up in 30 seconds or less, yeehaw!\"\n",
      "\n",
      "45 seconds remaining:\n",
      "\"Time's tickin', partner! Don't make me have to say it again: TWO MINUTES OR LESS! Get to the point before I rope you into silence!\"\n",
      "\n",
      "20 seconds remaining:\n",
      "\"Aww, shucks! You're fixin' to go over time like a runaway stagecoach! Cut to the chase, pronto! We ain't got all day, y'all!\"\n",
      "\n",
      "0 seconds remaining:\n",
      "\"Whoa, partner! Time's up! You're done! (Just kiddin', but seriously, wrap it up already!)\"\n",
      "Partner! I've got just what you need.\n",
      "\n",
      "At 2 minutes:\n",
      "\"Hey, don't get lassoed by the clock! Stay focused and finish strong!\"\n",
      "\n",
      "Remaining time: 120 seconds\n",
      "\n",
      "---\n",
      "\n",
      "At 1 minute:\n",
      "\"Giddy up, partner! You're running low on time. Get to the point already!\"\n",
      "\n",
      "Remaining time: 60 seconds\n",
      "\n",
      "---\n",
      "\n",
      "At 30 seconds:\n",
      "\"Hold up, pardner! Time's a-tickin' away! Wrap it up or risk gettin' left in the dust!\"\n",
      "\n",
      "Remaining time: 30 seconds\n",
      "\n",
      "---\n",
      "\n",
      "At 10 seconds:\n",
      "\"Whoa, partner! You're fixin' to run out of time! Make every second count... or you'll be eatin' dust!\"\n",
      "\n",
      "Remaining time: 10 seconds\n",
      "Partner! Here are my responses:\n",
      "\n",
      "* 66-60 seconds: \"Giddy up, partner! You've got plenty of time to ride out that update. Don't lasso the clock!\"\n",
      "remaining time: 65 seconds\n",
      "\n",
      "* 59-40 seconds: \"Time's a-wastin', partner! Keep your update concise like a cowboy's word!\"\n",
      "remaining time: 58 seconds\n",
      "\n",
      "* 39-20 seconds: \"Hold up, pardner! You're running low on time. Get to the point or get left behind!\"\n",
      "remaining time: 19 seconds\n",
      "\n",
      "* 19 seconds and under: \"HOLD UP, PARTNER! YOU'RE OUT OF TIME! Wrap it up, quick like a rattlesnake's strike!\"\n",
      "remaining time: 5 seconds\n",
      "Shucks! I reckon it's high time someone reminded folks about keepin' their updates short and sweet!\n",
      "\n",
      "*remaining time: 40 seconds*\n",
      "\n",
      "\"Partner, don't make me lasso you outta here! Two minutes is like a wild mustang - fast, fierce, and over before you know it. Keep it brief, or I'll have to rustle up some coffee for everyone... including yourself!\"\n",
      "\n",
      "(And so on, getting more urgent as the time remaining decreases!)\n",
      "Partner! Here are my responses:\n",
      "\n",
      "16 seconds remaining:\n",
      "\"Time's a-wastin', friend! Get on with it, y'all!\"\n",
      "\n",
      "12 seconds remaining:\n",
      "\"Hold up, partner! Two minutes, not two hours! Wrap it up, pronto!\"\n",
      "\n",
      "6 seconds remaining:\n",
      "\"Whoa, slow down! You're runnin' out of time like a wild mustang! Finish strong, or finish now!\"\n"
     ]
    }
   ],
   "source": [
    "duration = 120  # Duration in seconds\n",
    "run_api_call_loop(duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaa9d18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
